\documentclass{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

% ===== PACKAGES =====
\usepackage{amsmath,amssymb}
\usepackage{color}
\usepackage{subfigure}
\usepackage{mdframed}
\usepackage{changepage}
\newmdenv[
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep
]{siderules}
\renewcommand{\abstractname}{}

% ===== VARIABLES =====
\def \R{\mathbb{R}}
\def \Pr{\mathbb{P}}
\def \D{{\rm D}}
\def \N{{\rm N}}
\def \xx{{\boldsymbol{\rm x}}}
\def \y{{\rm y}}




% ===== HEADER BOX =====
\newcommand{\lecture}[2]{
\pagestyle{myheadings}
\thispagestyle{plain}
\newpage
\noindent
\begin{center}
\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal line
\rule{\textwidth}{0.4pt}\\[1\baselineskip] % Thin horizontal line
\vbox{\vspace{2mm}
\hbox to 6.28in { {\bf CS 760: Machine Learning} \hfill Spring 2024 }
\vspace{4mm}
\hbox to 6.28in { {\Large \hfill #1  \hfill} }
\vspace{4mm}
\hbox to 6.28in { {\scshape Authors:}  #2 \hfill }}
\vspace{-2mm}
\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal line
\rule{\textwidth}{1.6pt}\\[\baselineskip] % Thick horizontal line
\end{center}
\vspace*{4mm}
}

% ===== Jed's Defined Stuff ======
\DeclareMathOperator*{\argmin}{arg\!\min}
\DeclareMathOperator*{\argmax}{arg\!\max}
\usepackage{siunitx}
\usepackage{enumitem} % used to make alphabetical lists instead of numbered ones
\usepackage{mathtools}

% =============== DOCUMENT ===============
\begin{document}
\lecture{Homework 3: Logistic Regression}{Jed Pulley}

\begin{center}
{\Large {\sf \underline{\textbf{DO NOT POLLUTE!}} AVOID PRINTING, OR PRINT 2-SIDED MULTIPAGE.}}
\end{center}

\section*{Problem 3.1}
\begin{enumerate}[label=(\alph*)]
  \item I used good ol' fashioned guess and check. I set the iterations to 3000 and printed out the weights after every 100 iterations. Once the values started to level off, I considered that convergence
  \item It took me about 2000 iterations to converge to a respectable value, although extending it to 3000 refined it just a bit more.
  \item I performed min-max normalization to avoid running into a RuntimeWarning (when the exponential got WAY too big), so my results for $\hat{\theta}$ were: \[[-0.10924571, 0.03981236, -0.77742697, -0.01827487, 0.002808]\]
  \item The maximum log-likelihood of $\hat{\theta}$ is: -0.6700727101571831
  \item From Theorem 6.2 in the Logistic Regression notes, we can see that $\hat{\theta} \xlongrightarrow[\text{}]{\text{d}} \mathcal{N}(\theta^*, I^{-1}_{\theta^*})$ where the Fisher Information is shown as: \[I_{\theta^*} = \sum_{i=1}^{N}\frac{e^{-\theta^{*T}\sf{x}_i}}{(1 + e^{-\theta^{*T}\sf{x}_i})^2}\sf{x}_i\sf{x}^{\sf{T}}_i\]
\end{enumerate}

\section*{Problem 3.2}
\begin{enumerate}[label=(\alph*)]
  \item Borrowing from the Logistic Regression notes again, we can see that the MLE of the log-odds $\hat{\omega} := \hat{\theta}^{\sf{T}}\sf{x}$ where $\hat{\theta}$ are the true parameters, $\theta^*$.
  \item Furthermore, the asymptotic distribution of $\hat{\omega}$ is defined as $\hat{\omega} \xlongrightarrow[\text{}]{\text{d}} \mathcal{N}(\theta^{*\sf{T}}\textbf{x}, \textbf{x}^{\sf{T}}I^{-1}_{\theta^*}\textbf{x})$
\end{enumerate}

\section*{Problem 3.3}
\begin{enumerate}[label=(\alph*)]
  \item 
  \item 
  \item 
\end{enumerate}

\section*{Problem 3.4}
\begin{enumerate}[label=(\alph*)]
  \item 
  \item 
  \item 
\end{enumerate}

\end{document} 
































