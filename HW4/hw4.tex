\documentclass{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

% ===== PACKAGES =====
\usepackage{amsmath,amssymb}
\usepackage{color}
\usepackage{subfigure}
\usepackage{mdframed}
\usepackage{changepage}
\newmdenv[
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep
]{siderules}
\renewcommand{\abstractname}{}

% ===== VARIABLES =====
\def \R{\mathbb{R}}
\def \Pr{\mathbb{P}}
\def \D{{\rm D}}
\def \N{{\rm N}}
\def \xx{{\boldsymbol{\rm x}}}
\def \y{{\rm y}}




% ===== HEADER BOX =====
\newcommand{\lecture}[2]{
\pagestyle{myheadings}
\thispagestyle{plain}
\newpage
\noindent
\begin{center}
\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal line
\rule{\textwidth}{0.4pt}\\[1\baselineskip] % Thin horizontal line
\vbox{\vspace{2mm}
\hbox to 6.28in { {\bf CS 760: Machine Learning} \hfill Spring 2024 }
\vspace{4mm}
\hbox to 6.28in { {\Large \hfill #1  \hfill} }
\vspace{4mm}
\hbox to 6.28in { {\scshape Authors:}  #2 \hfill }}
\vspace{-2mm}
\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal line
\rule{\textwidth}{1.6pt}\\[\baselineskip] % Thick horizontal line
\end{center}
\vspace*{4mm}
}


% ===== Jed's Defined Stuff ======
\DeclareMathOperator*{\argmin}{arg\!\min}
\DeclareMathOperator*{\argmax}{arg\!\max}
\usepackage{siunitx}
\usepackage{enumitem} % used to make alphabetical lists instead of numbered ones
\usepackage{mathtools}
\usepackage{graphicx}

% =============== DOCUMENT ===============
\begin{document}
\lecture{Homework 4: Decision Trees}{Jed Pulley}

\begin{center}
{\Large {\sf \underline{\textbf{DO NOT POLLUTE!}} AVOID PRINTING, OR PRINT 2-SIDED MULTIPAGE.}}
\end{center}

\section*{Problem 4.1}
\textbf{Passenger Class:} First class is 1 and other classes are 0. I figured there are marginal benefits after 1st class. \\
\textbf{Sex:} I left sex alone, since it was already binary. \\
\textbf{Age:} I split age based on if you were a minor or not (i.e. age $<$ 18yo). \\
\textbf{Siblings/Spouse:} I chose 1 for no siblings/spouses and 0 for any number of them. My thought that is that any number of children, be it one or many, has a similar affect \\
\textbf{Parents/Children:} Similarly, I chose 1 for no parents/children and 0 for any \\
\textbf{Fare:} I split fare based on the median, if you are above, you get 1, otherwise 0.

\section*{Problem 4.2}
See $decision\_trees.ipynb$ under section 4.2 for code

Mutual Information of Features x1-x6 (rounded 5 digits): \\
x1: 0.05727 \\
x2: 0.21685 \\
x3: 0.00669 \\
x4: 0.00924 \\
x5: 0.01504 \\
x6: 0.05510

\section*{Problem 4.3}
See $decision\_trees.ipynb$ under section 4.3 for code

Two variables, $max\_depth$ and $min\_samples\_split$, are defined as my  stopping conditions. If a node exceeds the max depth, then stop. Otherwise, if a node is about to be split more times than our minimum sample split, then it will stop as well. This prevents the tree from getting too deep or wide.

\section*{Problem 4.4}
\includegraphics[scale=0.4]{4-7-images/tree_0}

\section*{Problem 4.5}
After running CV, I come up with an accuracy of around 89\%

\section*{Problem 4.6}
For a man paying a low fare with no children, siblings and parents on board, I would not have survived the titanic.

\section*{Problem 4.7}
\begin{enumerate}[label=(\alph*)]
  \item Notably, no matter how I sampled the 80\% of my data, all the subsets produced similar trees. \\
  \includegraphics[scale=0.4]{4-7-images/tree_0} \\
  \includegraphics[scale=0.4]{4-7-images/tree_1} \\
  \includegraphics[scale=0.4]{4-7-images/tree_2} \\
  \includegraphics[scale=0.4]{4-7-images/tree_3} \\
  \includegraphics[scale=0.4]{4-7-images/tree_4}
  \item Using 10-fold cross validation, I get an accuracy of 89\%.
  \item Using the same feature from before, I get the same results and do not survive, unfortunately.
\end{enumerate}

\section*{Problem 4.8}
\begin{enumerate}[label=(\alph*)]
  \item
  \includegraphics[scale=0.4]{4-8-images/tree_0} \\
  \includegraphics[scale=0.25]{4-8-images/tree_1} \\
  \includegraphics[scale=0.4]{4-8-images/tree_2} \\
  \includegraphics[scale=0.4]{4-8-images/tree_3} \\
  \includegraphics[scale=0.4]{4-8-images/tree_4} \\
  \includegraphics[scale=0.4]{4-8-images/tree_5}
  \item My accuracy reduced, but not by a lot. It went down to 86\%.
  \item No, I would not have survived. This is using the same feature as the previous two.
\end{enumerate}

\section*{Problem 4.9}
My decision tree predictions agree, but those disagree with my logistic regression predictions. My assumption as to why is because I'm binarizing my data here and I potentially lose information in the process. I would prefer to use logistic regression, in no small part because I find it far simpler to implement and understand.

\section*{Problem 4.10}
From the definition of mutual information, we know that:
\[I(x;y) = H(x) - H(x|y)\]

So, to prove that $I(x;y) = I(y;x)$, we need to show:
\[H(x) - H(x|y) = H(y) - H(y|x)\]

Conditional entropy is defined as:
 \[H(x|y) = H(x,y) - H(y) \text{ and, conversely } H(y|x) = H(y,x) - H(x)\]


Substituting these equations into the above, we get:
\[H(x) - (H(x,y) - H(y)) = H(y) - (H(y, x) - H(x))\]

After rearranging, we get:
\[H(x) + H(y) - H(x,y) = H(x) + H(y) - H(y, x)\]

To clean up, we subtract $H(y)$ and $H(x)$ from both sides, multiply by -1, and get:
\[H(x,y) = H(y,x)\]

This still necessitates that we show that joint entropy is symmetric, so we define joint entropy as:
\[ H(X,Y) = -\sum_{x\in X} \sum_{y \in Y} \mathbb{P}(x,y)log_2[\mathbb{P}(x,y)]\]
\[ H(Y,X) = -\sum_{y \in Y} \sum_{x\in X} \mathbb{P}(y,x)log_2[\mathbb{P}(y,x)]\]

Since the order of summation doesn't affect the result and we know that joint probability is symmetric, we can say prove that $H(x,y) = H(y,x) $

\end{document} 